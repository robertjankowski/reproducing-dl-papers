{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-0e08d007cbda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt-polaritydata/rt-polaritydata/rt-polarity.pos rt-polaritydata/rt-polaritydata/rt-polarity.neg\n"
     ]
    }
   ],
   "source": [
    "path = \"rt-polaritydata/rt-polaritydata/\"\n",
    "\n",
    "pos_path = os.path.join(path, 'rt-polarity.pos')\n",
    "neg_path = os.path.join(path, 'rt-polarity.neg')\n",
    "print(pos_path, neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(path, is_pos=True):\n",
    "    with open(path) as f:\n",
    "        review = pd.DataFrame({'review':f.read().splitlines()})\n",
    "    review['sentiment'] = 1 if is_pos else 0\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  the rock is destined to be the 21st century's ...          1\n",
       "1  the gorgeously elaborate continuation of \" the...          1\n",
       "2                     effective but too-tepid biopic          1\n",
       "3  if you sometimes like to go to the movies to h...          1\n",
       "4  emerges as something rare , an issue movie tha...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_review = load_review(pos_path, is_pos=True)\n",
    "neg_review = load_review(neg_path, is_pos=False)\n",
    "\n",
    "all_reviews = pd.concat([pos_review, neg_review])\n",
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\studia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_reviews.review_split = all_reviews.review.map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x2837fcc3cc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300517559051514),\n",
       " ('monarch', 0.6454660892486572),\n",
       " ('princess', 0.6156250834465027),\n",
       " ('crown_prince', 0.5818676352500916),\n",
       " ('prince', 0.577711820602417),\n",
       " ('kings', 0.5613664388656616),\n",
       " ('sultan', 0.5376776456832886),\n",
       " ('Queen_Consort', 0.5344247817993164),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queen = model['king'] - model['man'] + model['woman']\n",
    "model.similar_by_word(queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabluary size: 21425 words\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "def create_vocabulary(df):\n",
    "    splitted_reviews = df.review.map(lambda line: line.split())\n",
    "    vocab = set(reduce(operator.add, splitted_reviews))\n",
    "    return list(vocab)\n",
    "\n",
    "vocab = create_vocabulary(all_reviews)\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print('Total vocabluary size: {} words'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_number = {vocab[i]: i for i in range(VOCAB_SIZE)}\n",
    "number_to_word = {i:vocab[i] for i in range(VOCAB_SIZE)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_splitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, rock, is, destined, to, be, the, 21st, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, gorgeously, elaborate, continuation, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "      <td>[effective, but, too-tepid, biopic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[if, you, sometimes, like, to, go, to, the, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[emerges, as, something, rare, ,, an, issue, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  the rock is destined to be the 21st century's ...          1   \n",
       "1  the gorgeously elaborate continuation of \" the...          1   \n",
       "2                     effective but too-tepid biopic          1   \n",
       "3  if you sometimes like to go to the movies to h...          1   \n",
       "4  emerges as something rare , an issue movie tha...          1   \n",
       "\n",
       "                                     review_splitted  \n",
       "0  [the, rock, is, destined, to, be, the, 21st, c...  \n",
       "1  [the, gorgeously, elaborate, continuation, of,...  \n",
       "2                [effective, but, too-tepid, biopic]  \n",
       "3  [if, you, sometimes, like, to, go, to, the, mo...  \n",
       "4  [emerges, as, something, rare, ,, an, issue, m...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = all_reviews.copy()\n",
    "df[\"review_splitted\"] = df.review.map(lambda line: line.split())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review_enc\"] = df.review_splitted.map(lambda line: \\\n",
    "                                          [word_to_number[word] for word in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset_tf, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8529, (8529,), 2133, (2133,))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.review_enc.values.tolist()\n",
    "y = df.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "len(X_train), y_train.shape, len(X_test), y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.ragged.constant(X_train), \n",
    "                                                    y_train.values))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((tf.ragged.constant(X_test), \n",
    "                                                   y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE) # it should be padded_batch ...\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: [ 249    4  277  309  560    6 6639 4574    2   12   31 7759 3525 2128\n",
      "   93 2306   43 2312 2527    6   30 1334 8044   24   10   16   10   17\n",
      "  977   30  815 3339   41  841 7911  376 7974 1923    6  607  219   44\n",
      "  242 1603   11 4329  102 2821 1139    2  969  161   51   18    4 5844\n",
      " 2820  123 4493   40    6 4571   13  117   35  289  850  455   50  460\n",
      " 6359 1069  343   20    1 3733 3511 7670    3  147    4  336    2   42\n",
      "   18    4 3422  409 3533  871 2836  311    5 5080 1209    3  183  117\n",
      "   35 1187    5 1955   11    1  226 7745    3  183 1466 7359 7961 1466\n",
      "  665    2 6854 3178 1377 6266 1447  297    2 5797   36 4740  847 8050\n",
      "    2    5 1929 1631 5986   22 5541 5688    5 1036 3746 8050    3   69\n",
      "  264   35    4 4224    2   26   42   18    4  474 7968    8 1626   24\n",
      "   10   16   10   17  134   15    9    1  435   13    9   55  598 2357\n",
      "   48   30 6611 8044    3  500    1  101    6 2793    2 6548 7961 5861\n",
      "  311    9  139 5398 2270   11 3674 1642   25 7796 7961 5012    3  404\n",
      "    6    1  495 2632    5 5550  353    9  181  741    2   26 7297    6\n",
      "    1 7890 8043    2 6452 8050    2    5 6082  626 6854 3178 1064 8050\n",
      "    3   19 2227    9   56    4 1593    2 5366  194 2808  106    1   98\n",
      " 1451 1377   43    4 7757 1203  316   89    4 1334 1209    3  135 7968\n",
      "    8   33   81 6639 7837   44  208   24   10   16   10   17 7968 6548\n",
      " 7962 1251   25    1  108  778    2    9   97 7269 7961    7    1 1961\n",
      "    3   19 7483 2632    6 7255 5365    9 2125  652 4831  104    2   48\n",
      "    1 2880 1477   34 1992 1577 7961    7    1 6064  360 2387 1209 2356\n",
      "    3   19  724 1882 7974  177  256    1 1992 1577 7961    5 4022 5062\n",
      "  863    2   11   73    1 5758    9 6460  453  130    4 1668    2    4\n",
      " 2403  120  538 7968   21   31   93  193 1257    3 3371  713 6626   49\n",
      "    9   41 5234 7961 3864   24   10   16   10   17  128    1 3246  858\n",
      " 5550  353    9  114  224    2    1 2398  339 2493 3553 2975    6    1\n",
      " 2696    2    1 1183 8029    2 4967 1274    2 1036 3746   47    5 5541\n",
      " 5688   29   51 3339   56   20 3608   20   19 5057 7961   70   31 5324\n",
      " 8029   24   10   16   10   17  134  144   91    9 3498    3 3669 7961\n",
      " 1325 1093   47    9    4 7046 7961   20    1 5224   47 4296  967  633\n",
      " 1074    2 6279 6548 7961 4438   40   57 2408 2811 5848    7 1506  334\n",
      "   24   10   16   10   17 2130   33 6241 6007  124    2 5863 4685 1397\n",
      " 8045   95  546    4 4296 7961   48 7449 7968    8 2236 8045 7974  272\n",
      "   95  379    4  763 3347   47   22 1379 1004 7961   43  376 7974  652\n",
      " 4831   34  138   20    4   97   68 2902  899  102 3595   89   18 6639\n",
      "  413 7968    8  787    3  147    1 1334    2   42   18 2577 7878 6455\n",
      " 1344    2 5782   34   63    6   30  285 2754    7    1 3130  863    8\n",
      "   11 1147    7 3563 7961 1431   28  271   42   18  114 2135    5 6161\n",
      " 8029  158 3056 3950    8   11    1   65    2   42    9 3339   20  195\n",
      "    6    4  336 5803  810 7974 6770 8030    2    4  599    6 7028 2684\n",
      "  352 2040 6491  316   89    4 5147  125    6 1515    2    1 4812  139\n",
      "    4  933 7974 5568 1339 7974 5299  812 1621  166  475    3 4976 1858\n",
      "    2   67  214  763 3347  440   29  949   22 6639 4574 7968    8  787\n",
      "    3  404    6    1  107 2140 1588   29 5760 2073    3  768  302 6054\n",
      "   49    5 1665    9 2322 1139   36    7 4746  182 3733 1463 4662  353\n",
      " 6279   44    1  226  101 7033 2117   23 1515  120 4735 5210 2701  190\n",
      "    3 6071 7961    9  176 2145  360    7 1537    2  894 1383 6708   49\n",
      " 5788 1990   64 2736    8 1808 2153  100    2    5 2408 2811 5848  265\n",
      "    4 6720   64 5764 1868 7974 7689 2842   24   10   16   10   17 2478\n",
      " 1739    9 2097    2  271   53  110 1336  557 1692 1370 2089 3793    3\n",
      " 4312 2407  895 2026 6924   30 3870  124    3 3266   30    5   82 4370\n",
      "  211    9    1   45   11   73    1  386  327 6905  254  130   46   96\n",
      " 3972 1581  135 7968    8 1370 2089 3793 8050    2   26   39 7968    8\n",
      " 6639 7837  192    5 1884   24   10   16   10   17   12  117  186  136\n",
      "    3   12  117 2215  228    1  650 1771 7974 3642 8044    2    1 4129\n",
      " 7961    6    1 2186    5 2234    3  173    9 1382 7974  887  188   48\n",
      "   68 1127 4388 3305    8   89   37  117 3977   49    4 2797  565   24\n",
      "   10   16   10   17   12 3784   21    7  257   15  467 4854    2   26\n",
      " 8002 7968  111  861   68   52 6690  766  837 1076    3  699 3942   15\n",
      "   76   22  155 2400 3045 1064    3  684  314   57   93  193 5418 8029\n",
      " 7975], target: 1\n"
     ]
    }
   ],
   "source": [
    "for feature, target in train_dataset_tf.take(1):\n",
    "    print('Feature: {}, target: {}'.format(feature, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras.layers' has no attribute 'ToDense'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-2d0a0b0fdef6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model = tf.keras.Sequential([\n\u001b[0;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mragged\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.layers' has no attribute 'ToDense'"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(None,), dtype='int64', ragged=True),\n",
    "    tf.keras.layers.ToDense(pad_value=0, mask=True),\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)\n\nSecond structure: type=Tensor str=Tensor(\"embedding_1_input:0\", shape=(None, None), dtype=float32)\n\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"embedding_1_input:0\", shape=(None, None), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    317\u001b[0m     _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types,\n\u001b[1;32m--> 318\u001b[1;33m                                            expand_composites)\n\u001b[0m\u001b[0;32m    319\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)\n\nSecond structure: type=Tensor str=Tensor(\"embedding_1_input:0\", shape=(None, None), dtype=float32)\n\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"embedding_1_input:0\", shape=(None, None), dtype=float32)\" is not",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-7944b517869f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(train_dataset, epochs=10,\n\u001b[0;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                     validation_steps=30)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2495\u001b[0m     \u001b[0mflat_expected_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2496\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_expected_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2497\u001b[1;33m       \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[1;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[0;32m    323\u001b[0m                   \u001b[1;34m\"Entire first structure:\\n%s\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                   \u001b[1;34m\"Entire second structure:\\n%s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                   % (str(e), str1, str2))\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)\n\nSecond structure: type=Tensor str=Tensor(\"embedding_1_input:0\", shape=(None, None), dtype=float32)\n\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"embedding_1_input:0\", shape=(None, None), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n."
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-text (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-text\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-text # try on linux! and save trained model !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
